# Windows AI Assistant Configuration
# Copy this file to .env and customize for your setup
# IMPORTANT: Replace all <PLACEHOLDER> values with your own settings

# ============================================
# OLLAMA CONFIGURATION
# ============================================
# Ollama server address (running on your Windows workstation)
OLLAMA_HOST=127.0.0.1
OLLAMA_PORT=11434

# Model to use (available: llama3.2:3b, phi3, mistral, etc.)
# Download models with: ollama pull <model-name>
OLLAMA_MODEL=llama3.2:3b

# ============================================
# SPEECH RECOGNITION SETTINGS
# ============================================
# Wake word to activate the assistant
# This is what you say to start listening
# DEFAULT: "wolf-logic" (memorable, security-focused)
# CUSTOMIZE THIS: Change to something personal (e.g., "jarvis", "cortana", "activate")
WAKE_WORD=wolf-logic

# Use local Whisper for higher accuracy (true) or Windows native (false)
# Windows native: Fast, good accuracy, perfect UX flow (recommended for most)
# Whisper: Highest accuracy, requires more setup (Docker or WSL native)
USE_WHISPER=false

# Whisper model size (only if USE_WHISPER=true)
# Options: tiny, base, small, medium, large
# Larger = more accurate but slower and requires more VRAM
WHISPER_MODEL=base

# Whisper server URL (if running Whisper in Docker/WSL)
# Default: http://localhost:5000 (Whisper FastAPI server)
# Leave as-is unless you're running Whisper somewhere else
WHISPER_SERVER_URL=http://localhost:5000

# ============================================
# SECURITY - CONFIRMATION KEYWORD
# ============================================
# Keyword required to confirm system-changing commands
# This prevents accidental execution of high-risk operations like shutdown, delete
# DEFAULT: "wolf-logic" (same as wake word)
# CUSTOMIZE THIS: Set your own keyword for security (e.g., "authorize", "execute", "confirm")
#
# How it works:
# 1. You say: "wolf-logic, shutdown the computer"
# 2. Assistant asks: "This will shutdown Windows. Say '<your-confirmation-keyword>' to proceed."
# 3. You must explicitly say your confirmation keyword
# 4. Only then does the command execute
#
# This is YOUR security measure for critical operations. Make it unique to YOU.
CONFIRMATION_KEYWORD=wolf-logic

# ============================================
# TEXT-TO-SPEECH SETTINGS
# ============================================
# Speech rate (words per minute) - 200 is natural speech
TTS_RATE=200

# Optional: Specific voice to use
# Leave blank to use system default voice
TTS_VOICE=

# ============================================
# DIRECTML GPU ACCELERATION
# ============================================
# Enable DirectML for GPU acceleration on Windows
# Works with AMD, NVIDIA, and Intel GPUs
USE_DIRECTML=true

# GPU device ID (usually 0 for first GPU)
DIRECTML_DEVICE_ID=0

# ============================================
# SYSTEM CONTROL SETTINGS
# ============================================
# Allow the assistant to control system functions
# This enables: app launch, file ops, system commands, window management, etc.
ALLOW_SYSTEM_CONTROL=true

# Maximum length of voice commands (in characters)
MAX_COMMAND_LENGTH=1000

# ============================================
# NETWORKING - TAILSCALE
# ============================================
# Use Tailscale for remote access without firewall issues
USE_TAILSCALE=false

# If using Tailscale, set Ollama host to your Tailscale IP
# Example: OLLAMA_HOST=100.110.82.181 (your Tailscale IP)
# Leave as 127.0.0.1 if running locally on your workstation

# ============================================
# PERSISTENT MEMORY (Optional) - mem0 Integration
# ============================================
# Enable persistent memory learning (requires mem0 deployment)
ENABLE_PERSISTENT_MEMORY=false

# mem0 REST API endpoint for storing/retrieving memories
# Default: mem0 local deployment
MEM0_API_URL=https://mem0-api.complexsimplicity.com

# mem0 API authentication key
# CUSTOMIZE THIS: Get from your mem0 deployment admin
MEM0_API_KEY=<your-mem0-api-key-here>

# mem0 Retrieval Agent host (SSE streaming for real-time memory updates)
# Default: Local mem0 retrieval agent running in Docker
MEM0_RETRIEVAL_AGENT_HOST=100.110.82.181

# mem0 Retrieval Agent port (SSE stream on 5-second loop)
MEM0_RETRIEVAL_AGENT_PORT=8765

# Enable SSE streaming from retrieval agent (continuous memory context)
ENABLE_SSE_STREAMING=true

# Number of memories to retrieve per query (context limit)
MEMORY_CONTEXT_LIMIT=5

# ============================================
# LOGGING
# ============================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file location (relative to project root)
LOG_FILE=logs/assistant.log
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file location
LOG_FILE=logs/assistant.log

# ============================================
# ADVANCED OPTIONS
# ============================================
# Command timeout in seconds (how long pending commands wait for confirmation)
COMMAND_TIMEOUT=30

# Conversation history length to maintain for context
CONVERSATION_HISTORY_LENGTH=10

# Enable audio debug output
AUDIO_DEBUG=false