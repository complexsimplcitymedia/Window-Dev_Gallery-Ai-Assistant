version: '3.8'

# IMPORTANT: Architecture
# - Ollama: NATIVE on Windows (NOT Docker) - DirectML GPU acceleration
# - Whisper: NATIVE in WSL (NOT Docker) - ROCm GPU acceleration  
# - Python App: Can run anywhere (Windows, WSL, or Docker)
#
# All components connect via localhost networking:
# - Windows app → Ollama: http://127.0.0.1:11434
# - Windows app → Whisper: http://127.0.0.1:5000
# - WSL app → Ollama: http://host.docker.internal:11434 (if in Docker)
# - WSL app → Whisper: http://127.0.0.1:5000

services:
  # NOTE: Whisper is now run natively in WSL using Conda
  # See WHISPER_WSL_SETUP.md for setup instructions
  # Run with: conda activate whisper-server && python docker/whisper/whisper_server.py
  # No Docker container needed - native WSL provides better GPU support
  
  # This section is kept as reference for Docker-based Whisper (optional)
  # To use Docker Whisper instead of native WSL, uncomment below and adjust paths
  
  # whisper-server:
  #   build:
  #     context: ./docker/whisper
  #     dockerfile: Dockerfile
  #   container_name: ai-assistant-whisper
  #   ports:
  #     - "5000:5000"
  #   environment:
  #     - MODEL_SIZE=base
  #     - DEVICE=cpu
  #     - HOST=0.0.0.0
  #     - PORT=5000
  #     - LANGUAGE=en
  #   volumes:
  #     - ./models:/app/models
  #     - ./audio_temp:/app/audio_temp
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

volumes:
  whisper_models:
    driver: local